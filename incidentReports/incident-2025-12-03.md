# Incident: 2025-12-03 12-42-00

## Summary

> [!Note]
> Write a summary of the incident in a few sentences. Include what happened, why, the severity of the incident and how long the impact lasted.

Between the hours of 12:40 and 14:40 on 12/3/2025 for an average of 3 active users per minute which made about 170 total order attempts, completing an order failed. The event was triggered by the pizza factory service returning a 500 on every order until a support link reported in the factory error message was followed, starting at around 12:40. We are unsure why the pizza factory service began failing as it is a third party service, but pizza factory support was able to quickly resolve the issue once properly reported.

A bug in this code's factory request error cleanup masked the true underlying issue, making detecting, alerting, and debugging more difficult. This event was detected by Grafana monitoring. The teams started working on the event by locally reproducing the bug, temporarily removing the malfunctioning internal error handling code and therefore exposing the true factory error, and using the provided link in the error message to contact support. This Critical severity incident effect 100% of active users during incident period.

## Detection

> [!NOTE]
> When did the team detect the incident? How did they know it was happening? How could we improve time-to-detection? Consider: How would we have cut that time by half?

This incident was detected by the '500 Error' Grafana alert rule, though initially there was not enough traffic to cause the alert to fire, only to enter the 'pending' state. At 13:31, the alert finally triggered due to increased traffic. Kaden Keep was then paged and triage and debugging began. Time-to-detection could be improved by either scaling the alert threshold according to active users or by alerting to all 500 response errors immediately. For now, the alert has been set to fire on all 500 errors. Dynamic threshold scaling will be investigated by Kaden.

## Impact

> [!NOTE]
> Describe how the incident impacted internal and external users during the incident. Include how many support cases were raised.

This incident prevented any and all users from completing a pizza order with unclear error messages. It did not impact website browsability or menu viewing, but it did prevent any purchases. No support cases were raised.

## Timeline

> [!NOTE]
> Detail the incident timeline. We recommend using UTC to standardize for timezones.
> Include any notable lead-up events, any starts of activity, the first known impact, and escalations. Note any decisions or changed made, and when the incident ended, along with any post-impact events of note.

- _19:42_ - First detection of the Pizza Factory service returning a 500 error on pizza purchase request
- _20:29_ - First alert sent regarding error
- _20:30_ - Kaden acknowledge the alert, recognized a previously known rare error
- _21:25_ - Kaden started further investigating the known error
- _21:35_ - Kaden discovered that a new Factory error was causing this known error, increasing the known error frequency
- _21:40_ - Kaden removed the internal error and discovered the Factory error. Reported the error to Factory support through the link provided in the error message
- _21:44_ - All errors resolved

## Response

> [!NOTE]
> Who responded to the incident? When did they respond, and what did they do? Note any delays or obstacles to responding.

After receiving a page at 20:29 UTC, Kaden Keep came online at 21:25 UTC in Grafana.

This engineer originally recognized this error as a known, rare, low-severity issue which caused a delay in reaction. It was shortly discovered that a new underlying Pizza Factory service error was casuing this known internal issue to become more frequent.

## Root cause

> [!NOTE]
> Note the final root cause of the incident, the thing identified that needs to change in order to prevent this class of incident from happening again.

The third-party Pizza Factory service had an internal error in the system that required manual interaction with their support team using the included support link contained within the error response from the service. Additionally, the order error cleanup code contained a non-existant function call with a redundant cleanup purpose that prevented proper error reporting and handling.

## Resolution

> [!NOTE]
> Describe how the service was restored and the incident was deemed over. Detail how the service was successfully restored and you knew how what steps you needed to take to recovery.
> Depending on the scenario, consider these questions: How could you improve time to mitigation? How could you have cut that time by half?

The incident was resolved by first cleaning up the invalid code in the order error handler on a local instance of the service. From there, the underlying Pizza Factory error was discovered and support was contact to resolve the issue both locally and on production. The cleaned error handler code was then deployed to production and the service was tested to prove the errors no longer existed.

## Prevention

> [!NOTE]
> Now that you know the root cause, can you look back and see any other incidents that could have the same root cause? If yes, note what mitigation was attempted in those incidents and ask why this incident occurred again.

There are no other known related incidents, though this very well could easily become a repeated root cause. Proper testing should be done for all error handlers to ensure that if there are third party service errors again in the future that debugging will be able to be much faster.

## Action items

> [!NOTE]
> Describe the corrective action ordered to prevent this class of incident in the future. Note who is responsible and when they have to complete the work and where that work is being tracked.

1. Review all existing error handlers, particularly ones relating to third-party services. Cause failures to ensure the error handlers themselves do not contain errors. This will be performed by Kaden and will be tracked in GitHub Issues.
2. Configure a more intelligent error threshold scaling to better detect errors during low-traffic periods. This will be performed by Kaden and will be tracked in GitHub Issues.
